{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c238e0f1-3d8a-4522-b639-51755808bfc3",
   "metadata": {},
   "source": [
    "## Step 1: Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d42cc05d-f8c0-4d74-848a-8cd38f5caa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5b24d2-7de6-4604-a29b-4df3141402b3",
   "metadata": {},
   "source": [
    "## Step 2: Import Dataset and Perform Initial Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5b663ac-6749-40c4-83ac-d36175956d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from Excel to Pandas dataframe. Make sure to include PatientICN/Patientsid, PFT date, and ReportText columns\n",
    "df = pd.read_excel('[Insert Directory Here]/[Insert File Name Here].xlsx')\n",
    "\n",
    "# Convert ReportText column to string and remove carriage returns and extra spaces between words\n",
    "df['ReportText'] = df['ReportText'].astype('str')\n",
    "df['ReportText'] = df['ReportText'].str.replace(r'\\s+', ' ',regex=True).replace(r'\\n+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8494734-da4c-4964-833c-3f015ef32efa",
   "metadata": {},
   "source": [
    "## Step 3: Create Snippet Extraction Function after identifying template snippets with the following code:\n",
    "```\n",
    "n = 1\n",
    "for index,row in df.sample(frac=1)[:100].iterrows():\n",
    "    print(f\"Row Number: {n}\")\n",
    "    print(f\"Note Content: {row['ReportText']}\")\n",
    "    print(f\"PFT Date: {row['pft_date']}\")\n",
    "    print('-'*100)\n",
    "    n+=1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48094ecc-c730-497d-a91f-ead372312a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create snippet based on template start phrase(s)\n",
    "def extract_pft_context(text):\n",
    "    pattern = re.compile(r'Spirometry:.{0,500}|Spirometry Report.{0,1000}', re.IGNORECASE)\n",
    "    matches = pattern.findall(text)\n",
    "    return ''.join(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52881c4c-17f0-449a-a176-cd4e8ebe9bd2",
   "metadata": {},
   "source": [
    "## Step 4: Generate `Snippet` column by applying above function to the `ReportText` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752babce-891c-4487-9998-88d84d19485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the df to avoid warnings and run the ReportText column through the function to create the Snippet column\n",
    "df = df.copy(deep = True)\n",
    "df['Snippet'] = df['ReportText'].apply(extract_pft_context)\n",
    "\n",
    "# Create new dataframe where all rows with no snippet are dropped\n",
    "notes_with_fev = df[df['Snippet'] != ''].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a381a8b1-a3f3-4658-b6d3-2d80548351be",
   "metadata": {},
   "source": [
    "##### You can validate snippets with the following code:\n",
    "```\n",
    "n = 1\n",
    "for index,row in notes_with_fev.sample(frac=1)[:100].iterrows():\n",
    "    print(f\"Row Number: {n}\")\n",
    "    print(row['ReportText'])\n",
    "    print(row['pft_date'])\n",
    "    print('-'*100)\n",
    "    n+=1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859a0fc-e910-412b-ac7e-f720e7da3d1c",
   "metadata": {},
   "source": [
    "## Step 5: Initialize PFT Classification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98c8cb55-cc9f-4537-bf25-80577aa9f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pft(row):\n",
    "    fev1_abs_pre = []\n",
    "    fev1_abs_post = []\n",
    "    fev1_fvc_pre = []\n",
    "    fev1_fvc_post = []\n",
    "    fev1_perc_pred_pre = []\n",
    "    fev1_perc_pred_post = []\n",
    "    \n",
    "    text = row['Snippet']\n",
    "    \n",
    "    # FEV1 abs pre \n",
    "    fev1_abs_pre_pattern = re.compile(r'Pre/Best.*?FEV1.*?\\d*\\.\\d+\\s\\d*\\d.\\d+\\s(\\d*\\.\\d+)', re.IGNORECASE)\n",
    "    fev1_abs_pre_pattern_results = fev1_abs_pre_pattern.findall(text)\n",
    "    \n",
    "    if fev1_abs_pre_pattern_results:\n",
    "        fev1_abs_pre.append(fev1_abs_pre_pattern_results)\n",
    "        \n",
    "    # FEV1 abs post \n",
    "    fev1_abs_post_pattern = re.compile(r'Pre/Best.*?FEV1.*?\\d*\\.\\d+\\s\\d*\\d.\\d+\\s\\d*\\.\\d+\\s\\d{2}\\s(\\d*\\.\\d+)', re.IGNORECASE)\n",
    "    fev1_abs_post_pattern_results = fev1_abs_post_pattern.findall(text)\n",
    "    \n",
    "    if fev1_abs_post_pattern_results:\n",
    "        fev1_abs_post.append(fev1_abs_post_pattern_results)\n",
    "        \n",
    "    # FEV1 abs pre 2 \n",
    "    fev1_abs_pre_pattern_2 = re.compile(r'Pre/Base.*?FEV1.*?(\\d*\\.\\d+)', re.IGNORECASE)\n",
    "    fev1_abs_pre_pattern_2_results = fev1_abs_pre_pattern_2.findall(text)\n",
    "    \n",
    "    if fev1_abs_pre_pattern_2_results:\n",
    "        fev1_abs_pre.append(fev1_abs_pre_pattern_2_results)\n",
    "        \n",
    "    # FEV1 abs post 2 \n",
    "    fev1_abs_post_pattern_2 = re.compile(r'Pre/Base.*?FEV1.*?\\d*\\.\\d+\\s\\d{2}\\s\\d*\\.\\d+\\s(\\d*\\.\\d+)', re.IGNORECASE)\n",
    "    fev1_abs_post_pattern_2_results = fev1_abs_post_pattern_2.findall(text)\n",
    "    \n",
    "    if fev1_abs_post_pattern_2_results:\n",
    "        fev1_abs_post.append(fev1_abs_post_pattern_2_results)\n",
    "    \n",
    "    # FEV1/FVC pre 1 \n",
    "    fev1_fvc_pre_pattern = re.compile(r'Pre/Best.*?FEV1/FVC.*?\\d*\\.\\d+\\s\\d*\\d.\\d+\\s(\\d*\\.\\d+)', re.IGNORECASE)\n",
    "    fev1_fvc_pre_pattern_results = fev1_fvc_pre_pattern.findall(text)\n",
    "    \n",
    "    if fev1_fvc_pre_pattern_results:\n",
    "        fev1_fvc_pre.append(fev1_fvc_pre_pattern_results)\n",
    "        \n",
    "    # FEV1/FVC post 1 \n",
    "    fev1_fvc_post_pattern = re.compile(r'Pre/Best.*?FEV1/FVC.*?\\d*\\.\\d+\\s\\d*\\d.\\d+\\s\\d*\\.\\d+\\s\\d{2,3}\\s(\\d*\\.\\d+)', re.IGNORECASE)\n",
    "    fev1_fvc_post_pattern_results = fev1_fvc_post_pattern.findall(text)\n",
    "    \n",
    "    if fev1_fvc_post_pattern_results:\n",
    "        fev1_fvc_post.append(fev1_fvc_post_pattern_results)\n",
    "        \n",
    "    # FEV1/FVC pre 2 \n",
    "    fev1_fvc_pre_2_pattern = re.compile(r'Pre/Base.*?FEV1/FVC.*?(\\d{2,3})', re.IGNORECASE)\n",
    "    fev1_fvc_pre_2_pattern_results = fev1_fvc_pre_2_pattern.findall(text)\n",
    "    \n",
    "    if fev1_fvc_pre_2_pattern_results:\n",
    "        fev1_fvc_pre.append(fev1_fvc_pre_2_pattern_results)\n",
    "        \n",
    "    # FEV1/FVC post 2 \n",
    "    fev1_fvc_post_2_pattern = re.compile(r'Pre/Base.*?FEV1/FVC.*?\\d{2,3}\\s\\d{2,3}\\s\\d{2,3}\\s(\\d{2,3})', re.IGNORECASE)\n",
    "    fev1_fvc_post_2_pattern_results = fev1_fvc_post_2_pattern.findall(text)\n",
    "    \n",
    "    if fev1_fvc_post_2_pattern_results:\n",
    "        fev1_fvc_post.append(fev1_fvc_post_2_pattern_results)\n",
    "        \n",
    "    # FEV1 perc pred pre 1 \n",
    "    fev1_perc_pred_pre_1_pattern = re.compile(r'Pre/Best.*?FEV1.*?\\d*\\.\\d+\\s\\d*\\d.\\d+\\s\\d*\\.\\d+\\s(\\d{2})', re.IGNORECASE)\n",
    "    fev1_perc_pred_pre_1_pattern_results = fev1_perc_pred_pre_1_pattern.findall(text)\n",
    "    \n",
    "    if fev1_perc_pred_pre_1_pattern_results:\n",
    "        fev1_perc_pred_pre.append(fev1_perc_pred_pre_1_pattern_results)\n",
    "        \n",
    "    # FEV1 perc pred post 1 \n",
    "    fev1_perc_pred_post_1_pattern = re.compile(r'Pre/Best.*?FEV1.*?\\d*\\.\\d+\\s\\d*\\d.\\d+\\s\\d*\\.\\d+\\s\\d{2}\\s\\d*\\.\\d+\\s(\\d{2})', re.IGNORECASE)\n",
    "    fev1_perc_pred_post_1_pattern_results = fev1_perc_pred_post_1_pattern.findall(text)\n",
    "    \n",
    "    if fev1_perc_pred_post_1_pattern_results:\n",
    "        fev1_perc_pred_post.append(fev1_perc_pred_post_1_pattern_results)\n",
    "        \n",
    "    # FEV1 percent pred pre 2 \n",
    "    fev1_perc_pred_pre_2_pattern = re.compile(r'Pre/Base.*?FEV1.*?\\d*\\.\\d+\\s(\\d{2})', re.IGNORECASE)\n",
    "    fev1_perc_pred_pre_2_pattern_results = fev1_perc_pred_pre_2_pattern.findall(text)\n",
    "    \n",
    "    if fev1_perc_pred_pre_2_pattern_results:\n",
    "        fev1_perc_pred_pre.append(fev1_perc_pred_pre_2_pattern_results)\n",
    "        \n",
    "    # FEV1 percent pred post 2 \n",
    "    fev1_perc_pred_post_2_pattern = re.compile(r'Pre/Base.*?FEV1.*?\\d*\\.\\d+\\s\\d{2}\\s\\d*\\.\\d+\\s\\d*\\.\\d+\\s(\\d{2})', re.IGNORECASE)\n",
    "    fev1_perc_pred_post_2_pattern_results = fev1_perc_pred_post_2_pattern.findall(text)\n",
    "    \n",
    "    if fev1_perc_pred_post_2_pattern_results:\n",
    "        fev1_perc_pred_post.append(fev1_perc_pred_post_2_pattern_results)\n",
    "        \n",
    "    # FEV1 qualitative high\n",
    "    fev1_qual_high = []\n",
    "    \n",
    "    fev1_qual_high_pattern = re.compile(r'(spirometry: is normal)', re.IGNORECASE)\n",
    "    fev1_qual_high_pattern_result = fev1_qual_high_pattern.findall(text)\n",
    "    \n",
    "    if fev1_qual_high_pattern_result:\n",
    "        fev1_qual_high.append(fev1_qual_high_pattern_result)\n",
    "        \n",
    "    # FEV1 qualitative low\n",
    "    fev1_qual_low = []\n",
    "    \n",
    "    fev1_qual_low_pattern = re.compile(r'(mild obstructive ventilatory defect|moderate obstructive ventilatory defect|moderately severe obstructive ventilatory defect|moderate-severe obstructive ventilatory defect|severe obstructive ventilatory defect|very severe obstructive ventilatory defect)', re.IGNORECASE)\n",
    "    fev1_qual_low_pattern_result = fev1_qual_low_pattern.findall(text)\n",
    "    \n",
    "    if fev1_qual_low_pattern_result:\n",
    "        fev1_qual_low.append(fev1_qual_low_pattern_result)\n",
    "        \n",
    "    return pd.Series({'FEV1_Abs_Pre': fev1_abs_pre if fev1_abs_pre else None,\n",
    "                     'FEV1_Abs_Post': fev1_abs_post if fev1_abs_post else None,\n",
    "                      'FEV1_FVC_Pre': fev1_fvc_pre if fev1_fvc_pre else None,\n",
    "                      'FEV1_FVC_Post': fev1_fvc_post if fev1_fvc_post else None,\n",
    "                     'FEV1_Perc_Pred_Pre': fev1_perc_pred_pre if fev1_perc_pred_pre else None,\n",
    "                     'FEV1_Perc_Pred_Post': fev1_perc_pred_post if fev1_perc_pred_post else None,\n",
    "                     'FEV1_Qual_High': fev1_qual_high if fev1_qual_high else None,\n",
    "                     'FEV1_Qual_Low': fev1_qual_low if fev1_qual_low else None})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5887346-cb40-46e4-8d86-6d813f886f17",
   "metadata": {},
   "source": [
    "## Step 6: Run dataframe through the PFT extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44fbb83e-dc51-4300-9795-1faebfdbb6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = notes_with_pft.join(notes_with_pft.apply(classify_pft, axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bcdc18-3b47-4af4-9de9-4f866085c3d9",
   "metadata": {},
   "source": [
    "### Fix date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dd5140b-6c96-4cdc-baf5-f065bd3ae712",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['pft_date'] = results['pft_date'].apply(lambda x: x.strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffc1f3d-3908-402f-9afe-f12b7148204e",
   "metadata": {},
   "source": [
    "## Step 7: Extract values from FEV1 % predicted, FEV1:FVC pre-BD variables, and qualitative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc61bd7a-8a6b-4f57-9aed-e6c385f7168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_perc_pred_value(nested_list):\n",
    "    if nested_list is not None:\n",
    "        return int(nested_list[0][0])\n",
    "    \n",
    "# Create new variable 'FEV1_Perc_Pred' to hold extracted FEV1 % predicted quantitative value\n",
    "results['FEV1_Perc_Pred'] = results['FEV1_Perc_Pred_Pre'].apply(extract_perc_pred_value)\n",
    "\n",
    "def extract_fev1_fevc_value(nested_list):\n",
    "    if nested_list is not None:\n",
    "        return float(nested_list[0][0])\n",
    "\n",
    "# Create new variable 'fev1_fvc' to hold FEV1:FVC quantitative value\n",
    "results['fev1_fvc'] = results['FEV1_FVC_Pre'].apply(extract_fev1_fevc_value)\n",
    "\n",
    "def extract_fev1_qualitative(nested_list):\n",
    "    if nested_list is not None:\n",
    "        return str(nested_list[0][0])\n",
    "\n",
    "# Create new variables to hold qualitative data\n",
    "results['fev1_qual_neg'] = results['FEV1_Qual_Low'].apply(extract_fev1_qualitative)\n",
    "results['fev1_qual_pos'] = results['FEV1_Qual_High'].apply(extract_fev1_qualitative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d508e-93db-4b44-b86d-a918ba9247ca",
   "metadata": {},
   "source": [
    "## Step 8: Create mapping functions to map quantitative obstruction results to the correct classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d50800a-0341-4d3c-b9ee-68c97fd83b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fev1_severity(value):\n",
    "    if value >= 80:\n",
    "        return \"Normal\"\n",
    "    if 70 <= value <= 79:\n",
    "        return \"Mild\"\n",
    "    if 60 <= value <= 69:\n",
    "        return \"Moderate\"\n",
    "    if 50 <= value <= 59:\n",
    "        return \"Moderately Severe\"\n",
    "    if 35 <= value < 50:\n",
    "        return \"Severe\"\n",
    "    if value < 35:\n",
    "        return \"Very Severe\"\n",
    "\n",
    "def obstruction(value):\n",
    "    if value >= 70:\n",
    "        return \"Normal\"\n",
    "    if value < 70:\n",
    "        return \"Reduced\"\n",
    "    \n",
    "# Create new variables 'FEV1_Severity' and 'Obstruction', which hold the mapped values\n",
    "results['FEV1_Severity'] = results['FEV1_Perc_Pred'].map(fev1_severity)\n",
    "results['Obstruction'] = results['fev1_fvc'].map(obstruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d053e-0157-4c8d-8714-06d42764f75e",
   "metadata": {},
   "source": [
    "## Step 9a: Map qualitative values to FEV1 Severity if note did not contain extractable quantitative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "238f8712-5dce-49a1-aaed-50688b56dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fev1_severity_from_qual(row):\n",
    "    if row['FEV1_Severity'] is None and row['fev1_qual_pos'] == 'SPIROMETRY: Is normal':\n",
    "        return \"Normal\"\n",
    "    if row['FEV1_Severity'] is None and row['fev1_qual_neg'] == 'mild obstructive ventilatory defect':\n",
    "        return \"Mild\"\n",
    "    elif row['FEV1_Severity'] is None and row['fev1_qual_neg'] == 'moderate obstructive ventilatory defect':\n",
    "        return \"Moderate\"\n",
    "    elif row['FEV1_Severity'] is None and row['fev1_qual_neg'] in ['moderately severe obstructive ventilatory defect', 'moderate-severe obstructive ventilatory defect']:\n",
    "        return \"Moderately Severe\"\n",
    "    elif row['FEV1_Severity'] is None and row['fev1_qual_neg'] == 'severe obstructive ventilatory defect':\n",
    "        return \"Severe\"\n",
    "    elif row['FEV1_Severity'] is None and row['fev1_qual_neg'] == 'very severe obstructive ventilatory defect':\n",
    "        return \"Very Severe\"\n",
    "    else:\n",
    "        return row['FEV1_Severity']\n",
    "    \n",
    "# If quantitative data is missing for FEV1 % predicted, use available qualitative data to map value\n",
    "results['FEV1_Severity'] = results.apply(fev1_severity_from_qual, axis = 1)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe5b4ed-b3d0-4022-8554-fbbebb642772",
   "metadata": {},
   "source": [
    "## Step 9b: Map qualitative values to Obstruction if note did not contain extractable quantitative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86d95dab-b128-4784-81a3-7ed5ac4fe8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obstruction_from_qual(row):\n",
    "    if row['Obstruction'] is None and row['fev1_qual_pos'] == 'SPIROMETRY: Is normal':\n",
    "        return \"Normal\"\n",
    "    elif row['Obstruction'] is None and row['fev1_qual_neg'] is not None:\n",
    "        return \"Reduced\"\n",
    "    else:\n",
    "        return row['Obstruction']\n",
    "\n",
    "# If qunatitative data is missing for Obstruction, use available qualitative data to map value\n",
    "results['Obstruction'] = results.apply(obstruction_from_qual, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90276b3f-449f-417c-8997-8c9bbc750126",
   "metadata": {},
   "source": [
    "## Step 10: Drop duplicate rows and rows missing any extracted PFT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e02c71b7-1c18-46d9-9222-eb0fec802ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4134\n",
      "3498\n",
      "3345\n"
     ]
    }
   ],
   "source": [
    "# List columns to define on which variables to drop duplicates\n",
    "list_cols = ['Obstruction', 'FEV1_Severity', 'FEV1_Abs_Pre', 'FEV1_Perc_Pred_Pre', 'FEV1_FVC_Pre', 'FEV1_Abs_Post', 'FEV1_Perc_Pred_Post', 'FEV1_FVC_Post', 'FEV1_Qual_High', 'FEV1_Qual_Low']\n",
    "\n",
    "# Drop rows that have no extracted PFT values\n",
    "results = results.dropna(subset = list_cols, how='all')\n",
    "\n",
    "# Convert data types of columns in list_cols to string\n",
    "for col in list_cols:\n",
    "    results[col] = results[col].apply(lambda x: str(x))\n",
    "\n",
    "# Drop duplicates of PFT results based on columns of interest + PatientID and PFT date\n",
    "results = results.drop_duplicates(subset=['PatientICN', 'pft_date', 'Obstruction', 'FEV1_Severity', 'FEV1_Abs_Pre', 'FEV1_Perc_Pred_Pre', 'FEV1_FVC_Pre', 'FEV1_Abs_Post', 'FEV1_Perc_Pred_Post', 'FEV1_FVC_Post', 'FEV1_Qual_High', 'FEV1_Qual_Low'])\n",
    "\n",
    "# Replace cells with 'None' values to empty string for ease of readability in the output Excel file\n",
    "results.replace('None','',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cff138-526a-4beb-98f8-7f55db680381",
   "metadata": {},
   "source": [
    "## Step 11: Collapse notes from same PFT with multiple notes containing values for different variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23b19436-645f-4392-b0fd-ffc16690d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns that will keep the max value if the two rows being merged have different values for.\n",
    "columns_to_max = ['PatientSID', 'Obstruction', 'FEV1_Severity', 'FEV1_Abs_Pre', 'FEV1_Perc_Pred_Pre', 'FEV1_FVC_Pre', 'FEV1_Abs_Post', 'FEV1_Perc_Pred_Post', 'FEV1_FVC_Post', 'FEV1_Qual_High', 'FEV1_Qual_Low']\n",
    "\n",
    "# This function ensures that we don't lose one of the snippets upon the merge butu rather append them together.\n",
    "def concatenate_strings(series):\n",
    "    return ''.join(series.unique())\n",
    "\n",
    "# Define aggregation function to keep the  max value for columns that have different data in both rows\n",
    "agg_funcs = {col: 'max' for col in columns_to_max}\n",
    "\n",
    "# Create concatenated snippets for merged rows (instead of taking the \"max\" snippet value)\n",
    "agg_funcs['Snippet'] = concatenate_strings\n",
    "\n",
    "# Regenerate dataframe with the collapsed rows for identical PFTs with multiple notes\n",
    "results = results.groupby(['PatientICN','pft_date'], sort = False).agg(agg_funcs).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d67ef-38a2-438a-baac-2941da1a6248",
   "metadata": {},
   "source": [
    "## Step 12: Export dataframe to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a4b0440-71d4-4ca1-a628-18ce92e3ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns to export\n",
    "columns_to_export = ['Snippet', 'PatientICN', 'PatientSID', 'pft_date', 'Obstruction', 'FEV1_Severity', 'FEV1_Abs_Pre', 'FEV1_Perc_Pred_Pre', 'FEV1_FVC_Pre', 'FEV1_Abs_Post', 'FEV1_Perc_Pred_Post', 'FEV1_FVC_Post', 'FEV1_Qual_High', 'FEV1_Qual_Low']\n",
    "\n",
    "# Define desired output directory, file name, and file path\n",
    "output_dir = '[Insert Your Directory Here]/'\n",
    "file_name = '[Insert Your Filename Here].xlsx'\n",
    "full_path = output_dir + file_name\n",
    "\n",
    "# Export data as .xslx file\n",
    "results.to_excel(full_path, columns = columns_to_export, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
